{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning - Homework2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euSP1eJQfnq9",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/andikarachman/News-Title-Classification/blob/master/News_Title_Classification.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2ay-avMF6L6",
        "colab_type": "text"
      },
      "source": [
        "### Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KG1XlpFaaRG",
        "colab_type": "code",
        "outputId": "779a58a8-e8a7-4d1f-eb11-b52e21879053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "\n",
        "# Data analysis packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Deep learning packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Miscellaneous\n",
        "!pip install  -U bcolz\n",
        "import bcolz\n",
        "import pickle\n",
        "import re\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bcolz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
            "\r\u001b[K     |▎                               | 10kB 20.2MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 27.1MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 19.2MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 22.4MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 25.4MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 20.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 16.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 14.8MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 15.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 15.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 15.2MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 15.2MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 15.2MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 15.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 15.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 15.2MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 15.2MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 15.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194kB 15.2MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204kB 15.2MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 276kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 327kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 337kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 348kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 358kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 368kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 378kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 389kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 399kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 419kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 430kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 440kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 450kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████                      | 460kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 471kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 624kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 645kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 655kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 665kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 675kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 686kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 696kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 706kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 716kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 737kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 747kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 757kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 768kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 778kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 788kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 798kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 808kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 829kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 839kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 849kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 860kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 870kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 880kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 890kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 901kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 921kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 931kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 942kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 952kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 962kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 972kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 983kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 993kB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.0MB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.0MB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.0MB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1MB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1MB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1MB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1MB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1MB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.1MB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.1MB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1MB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.2MB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.2MB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.2MB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.2MB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.2MB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.2MB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.2MB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.3MB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3MB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3MB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3MB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3MB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3MB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.4MB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 15.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4MB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4MB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4MB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 15.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from bcolz) (1.18.3)\n",
            "Building wheels for collected packages: bcolz\n",
            "  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bcolz: filename=bcolz-1.2.1-cp36-cp36m-linux_x86_64.whl size=2663748 sha256=f9ada5f16062b3e5a369f6d51ffcab6115502b87258dbf22250981d1ce634bd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
            "Successfully built bcolz\n",
            "Installing collected packages: bcolz\n",
            "Successfully installed bcolz-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylpWwYQseFto",
        "colab_type": "code",
        "outputId": "2c32461d-a587-450a-d660-23cc761119e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# Loading data \n",
        "! git clone https://github.com/chaoyangzhengnash/ML2-Homework\n",
        "reference = pd.read_csv('ML2-Homework/reference.csv', sep=\",\", header = 'infer')\n",
        "test = pd.read_csv('ML2-Homework/test.csv', sep=\",\", header = 'infer')\n",
        "text = pd.read_csv('ML2-Homework/text.csv', sep=\",\", header = 'infer')\n",
        "train = pd.read_csv('ML2-Homework/train.csv', sep=\",\", header = 'infer')\n",
        "sample = pd.read_csv('ML2-Homework/sample.csv', sep=\",\", header = 'infer')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ML2-Homework'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 20 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (20/20), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tiKncwYh8IL",
        "colab_type": "code",
        "outputId": "119fbadb-95e9-4ec5-fa71-497793e49210",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "train_full = train.merge(text, how='left', left_on='id', right_on='id')\n",
        "# Drop weird rows\n",
        "train_full.drop(10218, inplace=True)\n",
        "\n",
        "test_full = test.merge(text, how='left', left_on='id', right_on='id')\n",
        "test_full['label'] = 99\n",
        "\n",
        "result = pd.concat([train_full, test_full],sort=False,ignore_index= True)\n",
        "train_full = result\n",
        "\n",
        "titles = train_full['title']\n",
        "labels = train_full['label']\n",
        "print(train_full.head())\n",
        "print(\"-------------------\")\n",
        "labels.value_counts()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id  label                                              title\n",
            "0   0      1  interactive visual exploration of neighbor bas...\n",
            "1   3      1  relational division four algorithms and their ...\n",
            "2   6      1  simplifying xml schema effortless handling of ...\n",
            "3   8      0  funbase a function based information managemen...\n",
            "4   9      0  inverted matrix efficient discovery of frequen...\n",
            "-------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99    12782\n",
              "0      2936\n",
              "1      2921\n",
              "2      2839\n",
              "3      2510\n",
              "4      1572\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7xQeh66GCbR",
        "colab_type": "text"
      },
      "source": [
        "### Data Processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_Dt_rR_FdZK",
        "colab_type": "code",
        "outputId": "d93ee7d4-3341-419c-923f-688d368eaa65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        }
      },
      "source": [
        "# Lowercase all words\n",
        "titles = titles.apply(lambda x: x.lower())\n",
        "# tokenize all titles in the data\n",
        "titles_token = titles.apply(lambda x: x.split())\n",
        "\n",
        "# Remove stop words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "titles_token2 = []\n",
        "\n",
        "for item in titles_token:\n",
        "    temp = []\n",
        "    for x in item:\n",
        "        if x not in stop_words:\n",
        "            temp.append(x)\n",
        "    titles_token2.append(temp)\n",
        "\n",
        "titles_token = pd.Series((i for i in titles_token2)) \n",
        "\n",
        "#a = list.index([s for s in enumerate(titles_token) if len(s) == 0])\n",
        "\n",
        "#titles_token = [s for s in titles_token if len(s) > 0]\n",
        "\n",
        "titles_token = pd.Series((i for i in titles_token)) \n",
        "\n",
        "print('Average word length of titles is {0:.0f}.'.format(np.mean(titles_token.apply(lambda x: len(x)))))\n",
        "print('Max word length of titles is {0:.0f}.'.format(np.max(titles_token.apply(lambda x: len(x)))))\n",
        "print('Min word length of titles is {0:.0f}.'.format(np.min(titles_token.apply(lambda x: len(x)))))\n",
        "print()\n",
        "print(\"--------- Distribution of word length of titles ---------\")\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "count = [len(title) for title in titles_token]\n",
        "print(pd.Series(count).value_counts())\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "print()\n",
        "print(\"--------- Words occurence ---------\")\n",
        "def track_vocab(sentences, verbose =  True):\n",
        "    \n",
        "    vocab = {}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except KeyError:\n",
        "                vocab[word] = 1\n",
        "                \n",
        "    return vocab\n",
        "# count the occurrence of all words in the data\n",
        "vocab_count = track_vocab(titles_token)\n",
        "print({k: vocab_count[k] for k in list(vocab_count)[:10]})\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Average word length of titles is 6.\n",
            "Max word length of titles is 22.\n",
            "Min word length of titles is 1.\n",
            "\n",
            "--------- Distribution of word length of titles ---------\n",
            "6     5134\n",
            "5     4905\n",
            "7     4150\n",
            "4     3174\n",
            "8     2806\n",
            "9     1728\n",
            "3     1448\n",
            "10     957\n",
            "11     453\n",
            "2      408\n",
            "12     211\n",
            "13     102\n",
            "14      37\n",
            "1       21\n",
            "17       8\n",
            "15       8\n",
            "16       7\n",
            "18       1\n",
            "20       1\n",
            "22       1\n",
            "dtype: int64\n",
            "\n",
            "--------- Words occurence ---------\n",
            "{'interactive': 220, 'visual': 129, 'exploration': 78, 'neighbor': 40, 'based': 2126, 'patterns': 209, 'data': 2204, 'streams': 164, 'relational': 472, 'division': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hINmmalM7p2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Encode the Data\n",
        "def create_lookup_tables(vocab_count):\n",
        "    \n",
        "    # sorting the words from most to least frequent in text occurrence\n",
        "    sorted_vocab = sorted(vocab_count, key=vocab_count.get, reverse=True)\n",
        "    # create vocab_to_int dictionary\n",
        "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
        "    \n",
        "    # return tuple\n",
        "    return (vocab_to_int, int_to_vocab)\n",
        "\n",
        "vocab_to_int, int_to_vocab = create_lookup_tables(vocab_count)\n",
        "\n",
        "# encode the data\n",
        "title_ints = []\n",
        "for title in titles_token:\n",
        "    title_ints.append([vocab_to_int[word] for word in title])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56cUhdus90vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_features(sentences_token, seq_length):\n",
        "    # getting the correct rows x cols shape\n",
        "    features = np.zeros((len(sentences_token), seq_length), dtype=int)\n",
        "\n",
        "    # for each title, I grab that title and \n",
        "    for i, row in enumerate(sentences_token):\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "    \n",
        "    return features\n",
        "\n",
        "# pad the titles\n",
        "seq_length = 22\n",
        "features = pad_features(title_ints, seq_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxRs-w6rHpdf",
        "colab_type": "text"
      },
      "source": [
        "###Define Training, Validation, and Test Set\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNtLS1qvl9wf",
        "colab_type": "code",
        "outputId": "e0e1314d-2cb5-4f70-a725-ffd22bb17144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "features"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ...,   117,     0,   158],\n",
              "       [    0,     0,     0, ...,  1246,    45,    44],\n",
              "       [    0,     0,     0, ...,  2075,   936,   446],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,    81,   135,    28],\n",
              "       [    0,     0,     0, ...,  1683,    53, 13158],\n",
              "       [    0,     0,     0, ...,   857,   153,  1710]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzBW9pb9HsPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pick up real test data \n",
        "features_a = features[0:12778]\n",
        "test_X_real = features[12778:len(labels)]\n",
        "features = features_a\n",
        "\n",
        "labels_a = labels[0:12778]\n",
        "test_y_real = labels[12778:len(labels)]\n",
        "labels = labels_a\n",
        "\n",
        "# Define Training, Validation, and Test Set\n",
        "\n",
        "train_X, val_test_X, train_y, val_test_y = train_test_split(features, labels, \n",
        "                                                            test_size=0.2, \n",
        "                                                            random_state=42, shuffle=True,\n",
        "                                                            stratify=labels)\n",
        "\n",
        "val_X, test_X, val_y, test_y = train_test_split(val_test_X, val_test_y, \n",
        "                                                test_size=0.5, \n",
        "                                                random_state=42, shuffle=True,\n",
        "                                                stratify=val_test_y)                                                      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J16G2X0SJD7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define data loaders \n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(np.asarray(train_X)), torch.from_numpy(np.asarray(train_y)))\n",
        "valid_data = TensorDataset(torch.from_numpy(np.asarray(val_X)), torch.from_numpy(np.asarray(val_y)))\n",
        "test_data = TensorDataset(torch.from_numpy(np.asarray(test_X)), torch.from_numpy(np.asarray(test_y)))\n",
        "test_data_real = TensorDataset(torch.from_numpy(np.asarray(test_X_real)), torch.from_numpy(np.asarray(test_y_real)))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "num_workers = 8\n",
        "\n",
        "# make sure to SHUFFLE the training data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
        "test_loader_real = DataLoader(test_data_real, shuffle=False, batch_size=batch_size, num_workers=num_workers)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3y3c_mc-PBK",
        "colab_type": "code",
        "outputId": "7545514f-7d2f-49d1-bff1-381c1c08ffca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 22])\n",
            "Sample input: \n",
            " tensor([[   0,    0,    0,  ..., 5958,  899,   13],\n",
            "        [   0,    0,    0,  ...,    8,   48,   28],\n",
            "        [   0,    0,    0,  ...,   10,  494,  482],\n",
            "        ...,\n",
            "        [   0,    0,    0,  ...,  299, 3017, 9247],\n",
            "        [   0,    0,    0,  ..., 2915, 1622,  872],\n",
            "        [   0,    0,    0,  ...,    2,  176, 1757]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([3, 3, 1, 0, 3, 3, 1, 2, 1, 2, 2, 0, 1, 1, 0, 0, 1, 2, 1, 3, 2, 4, 1, 0,\n",
            "        3, 1, 4, 3, 0, 4, 0, 4, 3, 2, 1, 1, 4, 0, 2, 4, 4, 2, 2, 4, 2, 2, 1, 0,\n",
            "        1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYeIHGBVVFrL",
        "colab_type": "text"
      },
      "source": [
        "### 5.0. Build Network Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0fp3Y09Xgxk",
        "colab_type": "code",
        "outputId": "a4842e18-239a-4116-b59f-2d1f8f10fa7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "# Preparation for glove\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-26 05:14:54--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-26 05:14:54--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-26 05:14:55--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.14MB/s    in 6m 30s  \n",
            "\n",
            "2020-04-26 05:21:26 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H52ILXwPZk2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "vectors = bcolz.carray(np.zeros(1), rootdir=f'glove.6B.300.dat', mode='w')\n",
        "\n",
        "with open(f'glove.6B.300d.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words.append(word)\n",
        "        word2idx[word] = idx\n",
        "        idx += 1\n",
        "        vect = np.array(line[1:]).astype(np.float)\n",
        "        vectors.append(vect)\n",
        "    \n",
        "vectors = bcolz.carray(vectors[1:].reshape((400000, 300)), rootdir=f'glove.6B.300.dat', mode='w')\n",
        "vectors.flush()\n",
        "pickle.dump(words, open(f'glove.6B.300_words.pkl', 'wb'))\n",
        "pickle.dump(word2idx, open(f'glove.6B.300_idx.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8YxaJG1HJfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a dictionary that given a word returns its vector\n",
        "vectors = bcolz.open(f'glove.6B.300.dat')[:]\n",
        "words = pickle.load(open(f'glove.6B.300_words.pkl', 'rb'))\n",
        "word2idx = pickle.load(open(f'glove.6B.300_idx.pkl', 'rb'))\n",
        "\n",
        "glove = {w: vectors[word2idx[w]] for w in words}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlGIpSeYB4I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create an embedding layer, that is a dictionary mapping integer indices (that represent words) to dense vectors\n",
        "sorted_vocab = sorted(vocab_count, key=vocab_count.get, reverse=True)\n",
        "target_vocab = sorted_vocab\n",
        "emb_dim = 300\n",
        "\n",
        "matrix_len = len(target_vocab)\n",
        "weights_matrix = np.zeros((matrix_len, emb_dim))\n",
        "words_found = 0\n",
        "\n",
        "for i, word in enumerate(target_vocab):\n",
        "    try: \n",
        "        weights_matrix[i] = glove[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
        "\n",
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    emb_layer.load_state_dict({'weight': torch.from_numpy(weights_matrix)})\n",
        "    if non_trainable:\n",
        "        emb_layer.weight.requires_grad = False\n",
        "\n",
        "    return emb_layer, num_embeddings, embedding_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0io6axDKCMua",
        "colab_type": "text"
      },
      "source": [
        "### Define RNN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98pB2PLIDFLs",
        "colab_type": "code",
        "outputId": "56ec0485-fd1f-4ef6-a6d7-1cc065725b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# First checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bddHZniYECSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# implementation of attention layer\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "        \n",
        "        self.supports_masking = True\n",
        "\n",
        "        self.bias = bias\n",
        "        self.feature_dim = feature_dim\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        \n",
        "        weight = torch.zeros(feature_dim, 1)\n",
        "        nn.init.kaiming_uniform_(weight)\n",
        "        self.weight = nn.Parameter(weight)\n",
        "        \n",
        "        if bias:\n",
        "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        feature_dim = self.feature_dim \n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = torch.mm(\n",
        "            x.contiguous().view(-1, feature_dim), \n",
        "            self.weight\n",
        "        ).view(-1, step_dim)\n",
        "        \n",
        "        if self.bias:\n",
        "            eij = eij + self.b\n",
        "            \n",
        "        eij = torch.tanh(eij)\n",
        "        a = torch.exp(eij)\n",
        "        \n",
        "        if mask is not None:\n",
        "            a = a * mask\n",
        "\n",
        "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
        "\n",
        "        weighted_input = x * torch.unsqueeze(a, -1)\n",
        "        return torch.sum(weighted_input, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruFcW_waH-Dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define of rnn\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weights_matrix, output_size, hidden_dim, drop_prob=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding layers\n",
        "        self.embedding, self.num_embeddings, self.embedding_dim = create_emb_layer(weights_matrix, True)\n",
        "        \n",
        "        # embedding dropout\n",
        "        self.dropout = nn.Dropout2d(drop_prob)\n",
        "        \n",
        "        # First lstm and GRU layers\n",
        "        self.lstm1 = nn.LSTM(self.embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.gru1 = nn.GRU(hidden_dim * 2, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        \n",
        "        # attention layer\n",
        "        self.attention = Attention(hidden_dim*2, seq_length)\n",
        "        \n",
        "        # Second lstm and GRU layers\n",
        "        self.lstm2 = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.gru2 = nn.GRU(hidden_dim * 2, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        \n",
        "        # linear\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 64)\n",
        "        self.out = nn.Linear(64, self.output_size)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some inputs.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embedding output\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        embeds = torch.squeeze(torch.unsqueeze(embeds, 0))\n",
        "        \n",
        "        # lstm, gru, and attention outputs\n",
        "        lstm_out, _ = self.lstm1(embeds)\n",
        "        gru_out, _ = self.gru1(lstm_out)\n",
        "        attention_out = self.attention(gru_out, 256)\n",
        "        attention_out = attention_out.view(batch_size, -1, self.hidden_dim * 2)\n",
        "        lstm_out, _ = self.lstm2(attention_out)\n",
        "        gru_out, _ = self.gru2(lstm_out)\n",
        "        \n",
        "        # linear outputs\n",
        "        out = gru_out.view(-1, gru_out.shape[2])\n",
        "        fc_out = self.relu(self.fc(out))\n",
        "        final_out = self.out(fc_out)\n",
        "    \n",
        "        return final_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2U_iEfEJBlq",
        "colab_type": "text"
      },
      "source": [
        "### # Instantiate the model w/ hyperparams\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMJHS605IV4C",
        "colab_type": "code",
        "outputId": "da0a8759-6c8c-4174-9073-8aa3f2fc108b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "weights_matrix = weights_matrix\n",
        "output_size = 5\n",
        "hidden_dim = 256\n",
        "\n",
        "net = RNN(weights_matrix, output_size, hidden_dim)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (embedding): Embedding(13159, 300)\n",
            "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "  (lstm1): LSTM(300, 256, batch_first=True, bidirectional=True)\n",
            "  (gru1): GRU(512, 256, batch_first=True, bidirectional=True)\n",
            "  (attention): Attention()\n",
            "  (lstm2): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
            "  (gru2): GRU(512, 256, batch_first=True, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=64, bias=True)\n",
            "  (out): Linear(in_features=64, out_features=5, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72kQUauQJLrA",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1n-Td-SNP8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_on_gpu = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JhHRIAuJVI1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "7bc5b94a-3936-474c-cec9-a799f63a5ec2"
      },
      "source": [
        "# Hyper params\n",
        "lr = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
        "epochs = 3\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip = 5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output = net(inputs)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        \n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output = net(inputs)\n",
        "                val_loss = criterion(output, labels)\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/3... Step: 100... Loss: 1.145034... Val Loss: 1.250795\n",
            "Epoch: 1/3... Step: 200... Loss: 1.167899... Val Loss: 1.082021\n",
            "Epoch: 2/3... Step: 300... Loss: 0.837045... Val Loss: 0.978574\n",
            "Epoch: 2/3... Step: 400... Loss: 0.834598... Val Loss: 0.878722\n",
            "Epoch: 3/3... Step: 500... Loss: 1.088658... Val Loss: 0.865168\n",
            "Epoch: 3/3... Step: 600... Loss: 0.761859... Val Loss: 0.817528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S5dyHTYP2mk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "70f3d9a3-f7a7-4371-bf1a-a9390d2cbdd0"
      },
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output = net(inputs)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output, labels)\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0, 1, 2, 3, 4)\n",
        "    pred = output.data.max(1, keepdim=True)[1]  \n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.886\n",
            "Test accuracy: 0.680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUSBlw8SVrIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMuSSufy1t8k",
        "colab_type": "text"
      },
      "source": [
        "### To get prediction for uploading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VBaWiVg2ODN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "preds = []\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader_real:\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output = net(inputs)\n",
        "    \n",
        "    # convert output probabilities to predicted class (0, 1, 2, 3, 4)\n",
        "    pred = output.data.max(1, keepdim=True)[1]  \n",
        "    preds.append(pred.numpy())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu2d2nTQdPZ_",
        "colab_type": "text"
      },
      "source": [
        "### save prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pX2fFiFcMi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds2 = np.concatenate( preds, axis=0 )\n",
        "test[\"label\"] = preds2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf227kPWnpq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.to_csv(r'preds.csv', index = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmAQI5UOobAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}